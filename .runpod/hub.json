{
  "title": "GPT-OSS 120B MXFP4 (Modular MAX) — Harmony + Responses",
  "description": "Serve OpenAI GPT-OSS‑120B in its native MXFP4 format on a single H100 using Modular MAX, with Harmony prompting and an OpenAI Responses-style interface (no Chat Completions adapter).",
  "type": "serverless",
  "category": "language",
  "iconUrl": "https://avatars.githubusercontent.com/u/9919?s=200&v=4",

  "config": {
    "runsOn": "GPU",
    "containerDiskInGb": 40,

    "gpuCount": 1,
    "gpuIds": "NVIDIA H100 80GB, NVIDIA A100 80GB, -NVIDIA RTX 4090",

    "allowedCudaVersions": [
      "12.8", "12.7", "12.6", "12.5", "12.4",
      "12.3", "12.2", "12.1", "12.0"
    ],

    "presets": [
      {
        "name": "H100 80GB (Modular MAX)",
        "defaults": {
          "ENGINE": "max",
          "MODEL_ID": "openai/gpt-oss-120b",
          "HF_HOME": "/models/hf_cache",
          "CUDA_VISIBLE_DEVICES": "0",
          "MAX_OUTPUT_TOKENS": 512
        }
      },
      {
        "name": "Transformers Fallback",
        "defaults": {
          "ENGINE": "transformers",
          "MODEL_ID": "openai/gpt-oss-120b",
          "HF_HOME": "/models/hf_cache",
          "CUDA_VISIBLE_DEVICES": "0",
          "MAX_OUTPUT_TOKENS": 512
        }
      }
    ],

    "env": [
      {
        "key": "ENGINE",
        "input": {
          "name": "Inference engine",
          "type": "string",
          "description": "Choose Modular MAX for best performance; Transformers is a portable fallback.",
          "options": [
            { "label": "Modular MAX", "value": "max" },
            { "label": "Transformers", "value": "transformers" }
          ],
          "default": "max"
        }
      },
      {
        "key": "MODEL_ID",
        "input": {
          "type": "huggingface",
          "name": "Hugging Face Model",
          "description": "Model on the Hugging Face Hub (must be Harmony-compatible).",
          "default": "openai/gpt-oss-120b"
        }
      },
      {
        "key": "HF_TOKEN",
        "input": {
          "name": "Hugging Face token",
          "type": "string",
          "description": "Required if the model repository needs authentication.",
          "advanced": true
        }
      },
      {
        "key": "HF_HOME",
        "input": {
          "name": "HF cache directory",
          "type": "string",
          "description": "Where to cache model files (mount a volume here for faster restarts).",
          "default": "/models/hf_cache",
          "advanced": false
        }
      },
      {
        "key": "MAX_OUTPUT_TOKENS",
        "input": {
          "name": "Max output tokens",
          "type": "number",
          "description": "Upper bound on tokens generated per response.",
          "min": 16,
          "max": 8192,
          "default": 512,
          "advanced": true
        }
      },
      {
        "key": "CUDA_VISIBLE_DEVICES",
        "input": {
          "name": "CUDA visible devices",
          "type": "string",
          "description": "GPU index selection mask.",
          "default": "0",
          "advanced": true
        }
      }
    ]
  }
}